{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 10-K Risk Section\n",
    "\n",
    "[CIK Lookup](https://www.sec.gov/cgi-bin/cik_lookup)\n",
    "\n",
    "* Microsoft - 0000789019\n",
    "* Tesla - 0001318605\n",
    "* Google, Inc - 0001288776\n",
    "* Walmart Stores - 0000217476\n",
    "* Amazon - 0001018724\n",
    "* Exxon Mobile - 0000034088\n",
    "* CVS Health - 0000064803\n",
    "* Berkshire Hathaway Corp - 0001067983\n",
    "* Alphabet, Inc - 0001652044\n",
    "\n",
    "[NASDAQ CSV](https://www.nasdaq.com/market-activity/stocks/screener)\n",
    "\n",
    "[S&P 500](https://en.wikipedia.org/wiki/S%26P_500)\n",
    "[S&P 500 Investopedia](https://www.investopedia.com/ask/answers/040215/what-does-sp-500-index-measure-and-how-it-calculated.asp)\n",
    "\n",
    "* The Standard and Poor's 500, or simply the S&P 500, is a stock market index tracking the stock performance of 500 large companies listed on exchanges in the United States.\n",
    "* It is one of the most commonly followed equity indices. As of December 31, 2020, more than $5.4 trillion was invested in assets tied to the performance of the index.\n",
    "* It is a free-float weighted/capitalization-weighted index\n",
    "\n",
    "* As of September 30, 2021, the nine largest companies on the list of S&P 500 companies accounted for 28.1% of the market capitalization of the index and were, in order of weighting, Apple, Microsoft,Alphabet (including both class A & C shares), Amazon.com, Meta Platforms, Tesla, Nvidia, Berkshire Hathaway and JPMorgan Chase\n",
    "* The components that have increased their dividends in 25 consecutive years are known as the S&P 500 Dividend Aristocrats.\n",
    "* In 2017, companies in the index derived on average 72% of their revenue in the United States.\n",
    "* The index is one of the factors in computation of the Conference Board Leading Economic Index, used to forecast the direction of the economy.\n",
    "* The index is associated with many ticker symbols, including ^GSPC, INX, and $SPX, depending on market or website.\n",
    "* The S&P 500 is maintained by S&P Dow Jones Indices, a joint venture majority-owned by S&P Global, and its components are selected by a committee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sec_cik_mapper import StockMapper\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "#Create the downloader\n",
    "# dl = Downloader()\n",
    "\n",
    "#Download 5 years of 10k documents\n",
    "# dl.get(\"10-K\", \"0001341439\", after=\"2014-01-01\", before=\"2022-07-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.environ[\"HOME\"], \"Datasets\",\n",
    "                        \"Financial\",\"SEC_10K\")\n",
    "\n",
    "ML_DATA_DIR = os.path.join(DATA_DIR, \"ML_Dataset\")\n",
    "\n",
    "TXT_10K_DIR = os.path.join(DATA_DIR, \"sec_10k_text_files\")\n",
    "# HTML_10K_DIR = os.path.join(DATA_DIR, \"sec_10k_html_files\")\n",
    "\n",
    "TXT_10K_FILES = sorted(glob(os.path.join(TXT_10K_DIR, \"*.txt\")))\n",
    "\n",
    "# HTML_10K_FILES = sorted(glob(os.path.join(HTML_10K_DIR, \"*.html\")))\n",
    "# HTML_10K_FILES[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test Sample Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_company = TXT_10K_FILES[10]\n",
    "# sample_company = HTML_10K_FILES[10]\n",
    "sample_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "single_filepath = '/Users/jonghang/Datasets/Financial/SEC_10K/sec_10k_text_files/0000001961_2017.txt'\n",
    "singlefile = os.path.basename(single_filepath)\n",
    "fileparts = singlefile.split('_')\n",
    "cik = fileparts[0]\n",
    "year = fileparts[1].replace('.txt','')\n",
    "print(cik, ' ', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "docs_in_sample_company = glob(os.path.join(sample_company, \"**/*.txt\"), recursive=True)\n",
    "\n",
    "#Read in text files\n",
    "sample_docs = []\n",
    "for single_file in TXT_10K_FILES[100:110]:\n",
    "    with open(single_file, 'rb') as f:\n",
    "        #Convert to html then read in the text piece\n",
    "        singlefile = os.path.basename(single_file)\n",
    "        fileparts = singlefile.split('_')\n",
    "        cik = fileparts[0]\n",
    "        year = fileparts[1].replace(\".txt\",\"\")\n",
    "        sample_docs.append(BeautifulSoup(f.read()).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check first document\n",
    "sample_text = sample_docs[2]\n",
    "print(sample_text[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare regular expression\n",
    "pattern = r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\\.{0,1})|(ITEM\\s(1A|1B|7A|7|8))'\n",
    "regex = re.compile(pattern)\n",
    "# Find matches\n",
    "matches = regex.finditer(sample_text)\n",
    "\n",
    "# Write a for loop to print the matches\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Matches\n",
    "matches = regex.finditer(sample_text)\n",
    "# Create the dataframe\n",
    "regex_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "regex_df.columns = ['item', 'start', 'end']\n",
    "regex_df['item'] = regex_df.item.str.upper()\n",
    "\n",
    "# Display the dataframe\n",
    "regex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get rid of unnesesary charcters from the dataframe\n",
    "regex_df.replace('&#160;',' ',regex=True,inplace=True)\n",
    "regex_df.replace('&nbsp;',' ',regex=True,inplace=True)\n",
    "regex_df.replace(' ','',regex=True,inplace=True)\n",
    "regex_df.replace('\\.','',regex=True,inplace=True)\n",
    "regex_df.replace('>','',regex=True,inplace=True)\n",
    "\n",
    "# display the dataframe\n",
    "regex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "pos_df = regex_df.sort_values('start', ascending=True).drop_duplicates(subset=['item'], keep='last')\n",
    "# Set item as the dataframe index\n",
    "pos_df.set_index('item', inplace = True)\n",
    "\n",
    "# Display the dataframe\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get Item 1a raw content\n",
    "item_1a_raw = sample_text[pos_df['start'].loc['ITEM1A']:pos_df['start'].loc['ITEM1B']]\n",
    "# Get Item 7 raw content\n",
    "item_7_raw = sample_text[pos_df['start'].loc['ITEM7']:pos_df['start'].loc['ITEM7A']]\n",
    "# Get Item 7a raw content\n",
    "item_7a_raw = sample_text[pos_df['start'].loc['ITEM7A']:pos_df['start'].loc['ITEM8']]\n",
    "\n",
    "item_1a_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Apply BeautifulSoup to refine the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create bs4 object from the raw text\n",
    "item_1a_content = BeautifulSoup(item_1a_raw, 'lxml')\n",
    "item_7_content = BeautifulSoup(item_7_raw, 'lxml')\n",
    "item_7a_content = BeautifulSoup(item_7a_raw, 'lxml')\n",
    "\n",
    "# Further organize into a proper tree structure by applying .pretiffy()\n",
    "print(item_1a_content.prettify()[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Our goal is though to remove html tags and see the content\n",
    "### Method get_text() is what we need, \\n\\n is optional, I just added this to read text\n",
    "### more cleanly, it's basically new line character between sections.\n",
    "print(item_1a_content.get_text(\"\\n\\n\")[0:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_text = item_1a_content.get_text().strip().replace('\\n',' ').replace('\\t', ' ')\n",
    "print(processed_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assemble Functions\n",
    "\n",
    "## Clean Text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(input_text):\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    input_text = stemmer.stem(input_text)\n",
    "    tokens = word_tokenize(input_text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = words[5:]\n",
    "    cleaned_sentense = ' '.join(words)\n",
    "    # the first 5 words are heading, remove them\n",
    "    return cleaned_sentense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assemble NLP Table for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_single_10k_table(input_cik, input_year, input_text):\n",
    "    # Write the regex\n",
    "    regex = re.compile(r'(>Item(\\s|&#160;|&nbsp;)(1A|1B|7A|7|8)\\.{0,1})|(ITEM\\s(1A|1B|7A|7|8))')\n",
    "    # Use finditer to math the regex\n",
    "    matches = regex.finditer(input_text)\n",
    "\n",
    "    # Create the dataframe\n",
    "    regex_df = pd.DataFrame([(x.group(), x.start(), x.end()) for x in matches])\n",
    "    regex_df.columns = ['item', 'start', 'end']\n",
    "    regex_df['item'] = regex_df.item.str.upper()\n",
    "\n",
    "    # clean up the dataframe by removing special characters\n",
    "    regex_df.replace('&#160;',' ',regex = True,inplace = True)\n",
    "    regex_df.replace('&nbsp;',' ',regex = True,inplace = True)\n",
    "    regex_df.replace(' ','',regex = True, inplace = True)\n",
    "    regex_df.replace('\\.','',regex = True, inplace = True)\n",
    "    regex_df.replace('>','',regex = True, inplace = True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    pos_df = regex_df.sort_values('start', ascending = True).drop_duplicates(subset = ['item'], keep = 'last')\n",
    "    # Set item as the dataframe index\n",
    "    pos_df.set_index('item', inplace = True)\n",
    "\n",
    "    # display the dataframe\n",
    "    # display(regex_df.head())\n",
    "\n",
    "    # Get Item 1a\n",
    "    item_1a_raw = input_text[pos_df['start'].loc['ITEM1A']:pos_df['start'].loc['ITEM1B']]\n",
    "    item_1a_content = BeautifulSoup(item_1a_raw, 'lxml')\n",
    "    processed_item1a = item_1a_content.get_text().strip().replace('\\n',' ').replace('\\t', ' ').lower()\n",
    "    cleaned_item1a = clean_text(processed_item1a)\n",
    "    # Get Item 7\n",
    "    item_7_raw = input_text[pos_df['start'].loc['ITEM7']:pos_df['start'].loc['ITEM7A']]\n",
    "    item_7_content = BeautifulSoup(item_7_raw, 'lxml')\n",
    "    processed_item7 = item_7_content.get_text().strip().replace('\\n',' ').replace('\\t', ' ').lower()\n",
    "    cleaned_item7 = clean_text(processed_item7)\n",
    "    # Get Item 7a\n",
    "    item_7a_raw = input_text[pos_df['start'].loc['ITEM7A']:pos_df['start'].loc['ITEM8']]\n",
    "    item_7a_content = BeautifulSoup(item_7a_raw, 'lxml')\n",
    "    processed_item7a = item_7a_content.get_text().strip().replace('\\n',' ').replace('\\t', ' ').lower()\n",
    "    cleaned_item7a = clean_text(processed_item7a)\n",
    "\n",
    "    single_dict = {\"CIK\":[input_cik], \"YEAR\": input_year, \"ITEM_1A\":[processed_item1a],\n",
    "                   \"ITEM_7\":[processed_item7], \"ITEM_7A\":[processed_item7a],\n",
    "                   \"CLEANED_ITEM_1A\":[cleaned_item1a],\n",
    "                   \"CLEANED_ITEM_7\":[cleaned_item7],\n",
    "                   \"CLEANED_ITEM_7A\":[cleaned_item7a]}\n",
    "    single_df_10k = pd.DataFrame(single_dict)\n",
    "    return single_df_10k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build ML Dataset\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def build_nlp_10k_ml_dataset(text_10k_filepaths, start_idx = 0):\n",
    "    cleaned_10k_dfs = []\n",
    "\n",
    "    for single_file in text_10k_filepaths[start_idx:]:\n",
    "        with open(single_file, 'rb') as f:\n",
    "            #Convert to html then read in the text piece\n",
    "            singlefile = os.path.basename(single_file)\n",
    "            fileparts = singlefile.split('_')\n",
    "            cik = fileparts[0]\n",
    "            year = fileparts[1].replace(\".txt\",\"\")\n",
    "            single_df = build_single_10k_table(cik,\n",
    "                                               year,\n",
    "                                               BeautifulSoup(f.read()).text)\n",
    "            cleaned_10k_dfs.append(single_df)\n",
    "    return cleaned_10k_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assemble Candidates ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CIK_YEAR</th>\n",
       "      <th>CIK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ITEM_1A</th>\n",
       "      <th>ITEM_7</th>\n",
       "      <th>ITEM_7A</th>\n",
       "      <th>CLEANED_ITEM_1A</th>\n",
       "      <th>CLEANED_ITEM_7</th>\n",
       "      <th>CLEANED_ITEM_7A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001637207_2020</td>\n",
       "      <td>0001637207</td>\n",
       "      <td>2020</td>\n",
       "      <td>item 1a.risk factorsthe following is a descrip...</td>\n",
       "      <td>item 7.       management’s discussion and anal...</td>\n",
       "      <td>item 7a.quantitative and qualitative disclosur...</td>\n",
       "      <td>risks inherent businesswe affected factors adv...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>exposure market risk includes fluctuating inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001300699_2018</td>\n",
       "      <td>0001300699</td>\n",
       "      <td>2018</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk exposure market risk includes fluctuating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001227500_2018</td>\n",
       "      <td>0001227500</td>\n",
       "      <td>2018</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk exposure market risk includes fluctuating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001653653_2020</td>\n",
       "      <td>0001653653</td>\n",
       "      <td>2020</td>\n",
       "      <td>item 1a.risk factorsthe following is a descrip...</td>\n",
       "      <td>item 7.       management’s discussion and anal...</td>\n",
       "      <td>item 7a.quantitative and qualitative disclosur...</td>\n",
       "      <td>risks inherent businesswe affected factors adv...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>exposure market risk includes fluctuating inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0000007323_2017</td>\n",
       "      <td>0000007323</td>\n",
       "      <td>2017</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk dollars millions exposure market risk inc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID_CIK_YEAR         CIK  YEAR  \\\n",
       "0  ID_0001637207_2020  0001637207  2020   \n",
       "0  ID_0001300699_2018  0001300699  2018   \n",
       "0  ID_0001227500_2018  0001227500  2018   \n",
       "0  ID_0001653653_2020  0001653653  2020   \n",
       "0  ID_0000007323_2017  0000007323  2017   \n",
       "\n",
       "                                             ITEM_1A  \\\n",
       "0  item 1a.risk factorsthe following is a descrip...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "0  item 1a.risk factorsthe following is a descrip...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "\n",
       "                                              ITEM_7  \\\n",
       "0  item 7.       management’s discussion and anal...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "0  item 7.       management’s discussion and anal...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "\n",
       "                                             ITEM_7A  \\\n",
       "0  item 7a.quantitative and qualitative disclosur...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "0  item 7a.quantitative and qualitative disclosur...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "\n",
       "                                     CLEANED_ITEM_1A  \\\n",
       "0  risks inherent businesswe affected factors adv...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "0  risks inherent businesswe affected factors adv...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "\n",
       "                                      CLEANED_ITEM_7  \\\n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "\n",
       "                                     CLEANED_ITEM_7A  \n",
       "0  exposure market risk includes fluctuating inte...  \n",
       "0  risk exposure market risk includes fluctuating...  \n",
       "0  risk exposure market risk includes fluctuating...  \n",
       "0  exposure market risk includes fluctuating inte...  \n",
       "0  risk dollars millions exposure market risk inc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TXT_10K_CANDIDATES_FILES = glob(os.path.join(DATA_DIR, \"sec_10k_text_files_CANDIDATES\",\"*.txt\"))\n",
    "# print(TXT_10K_CANDIDATES_FILES[:5])\n",
    "\n",
    "candidates_ml_dfs = build_nlp_10k_ml_dataset(TXT_10K_CANDIDATES_FILES, start_idx=0)\n",
    "candidates_ml_df = pd.concat(candidates_ml_dfs, axis = 0)\n",
    "\n",
    "# Create id column of CIK_YEAR\n",
    "\n",
    "candidates_ml_df.insert(loc = 0,\n",
    "                        column = \"ID_CIK_YEAR\",\n",
    "                        value = \"ID_\" + candidates_ml_df[\"CIK\"] + \"_\" + candidates_ml_df[\"YEAR\"])\n",
    "\n",
    "display(candidates_ml_df.shape)\n",
    "display(candidates_ml_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Write to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# candidates_ml_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_Candidates.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write NLTKly parsed data\n",
    "candidates_ml_cleaned_df = candidates_ml_df[[\"ID_CIK_YEAR\",\"CIK\",\"YEAR\",\n",
    "                                   \"CLEANED_ITEM_1A\",\"CLEANED_ITEM_7\",\n",
    "                                   \"CLEANED_ITEM_7A\"]]\n",
    "candidates_ml_cleaned_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_Candidates_Cleaned.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assemble S&P 500 ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2480, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_CIK_YEAR</th>\n",
       "      <th>CIK</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>ITEM_1A</th>\n",
       "      <th>ITEM_7</th>\n",
       "      <th>ITEM_7A</th>\n",
       "      <th>CLEANED_ITEM_1A</th>\n",
       "      <th>CLEANED_ITEM_7</th>\n",
       "      <th>CLEANED_ITEM_7A</th>\n",
       "      <th>SP_500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001335258_2020</td>\n",
       "      <td>0001335258</td>\n",
       "      <td>2020</td>\n",
       "      <td>item 1a.risk factorsthe following is a descrip...</td>\n",
       "      <td>item 7.       management’s discussion and anal...</td>\n",
       "      <td>item 7a.quantitative and qualitative disclosur...</td>\n",
       "      <td>risks inherent businesswe affected factors adv...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>exposure market risk includes fluctuating inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0000900075_2021</td>\n",
       "      <td>0000900075</td>\n",
       "      <td>2021</td>\n",
       "      <td>item 1a.risk factorsthe following is a descrip...</td>\n",
       "      <td>item 7.       management’s discussion and anal...</td>\n",
       "      <td>item 7a.quantitative and qualitative disclosur...</td>\n",
       "      <td>risks inherent business risks uncertainties de...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>exposure market risk includes fluctuating inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0000105770_2019</td>\n",
       "      <td>0000105770</td>\n",
       "      <td>2019</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk exposure market risk includes fluctuating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001283699_2019</td>\n",
       "      <td>0001283699</td>\n",
       "      <td>2019</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk exposure market risk includes fluctuating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0001137789_2018</td>\n",
       "      <td>0001137789</td>\n",
       "      <td>2018</td>\n",
       "      <td>item 1a.    risk factors              the foll...</td>\n",
       "      <td>item 7.    management's discussion and analysi...</td>\n",
       "      <td>item 7a.    quantitative and qualitative discl...</td>\n",
       "      <td>principal risks inherent business affected fac...</td>\n",
       "      <td>condition results operations dollars millions ...</td>\n",
       "      <td>risk exposure market risk includes fluctuating...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID_CIK_YEAR         CIK  YEAR  \\\n",
       "0  ID_0001335258_2020  0001335258  2020   \n",
       "0  ID_0000900075_2021  0000900075  2021   \n",
       "0  ID_0000105770_2019  0000105770  2019   \n",
       "0  ID_0001283699_2019  0001283699  2019   \n",
       "0  ID_0001137789_2018  0001137789  2018   \n",
       "\n",
       "                                             ITEM_1A  \\\n",
       "0  item 1a.risk factorsthe following is a descrip...   \n",
       "0  item 1a.risk factorsthe following is a descrip...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "0  item 1a.    risk factors              the foll...   \n",
       "\n",
       "                                              ITEM_7  \\\n",
       "0  item 7.       management’s discussion and anal...   \n",
       "0  item 7.       management’s discussion and anal...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "0  item 7.    management's discussion and analysi...   \n",
       "\n",
       "                                             ITEM_7A  \\\n",
       "0  item 7a.quantitative and qualitative disclosur...   \n",
       "0  item 7a.quantitative and qualitative disclosur...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "0  item 7a.    quantitative and qualitative discl...   \n",
       "\n",
       "                                     CLEANED_ITEM_1A  \\\n",
       "0  risks inherent businesswe affected factors adv...   \n",
       "0  risks inherent business risks uncertainties de...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "0  principal risks inherent business affected fac...   \n",
       "\n",
       "                                      CLEANED_ITEM_7  \\\n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "0  condition results operations dollars millions ...   \n",
       "\n",
       "                                     CLEANED_ITEM_7A  SP_500  \n",
       "0  exposure market risk includes fluctuating inte...       1  \n",
       "0  exposure market risk includes fluctuating inte...       1  \n",
       "0  risk exposure market risk includes fluctuating...       1  \n",
       "0  risk exposure market risk includes fluctuating...       1  \n",
       "0  risk exposure market risk includes fluctuating...       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TXT_10K_SP500_FILES = glob(os.path.join(DATA_DIR, \"sec_10k_text_files_SP500\",\"*.txt\"))\n",
    "# print(TXT_10K_CANDIDATES_FILES[:5])\n",
    "\n",
    "sp500_ml_dfs = build_nlp_10k_ml_dataset(TXT_10K_SP500_FILES, start_idx=0)\n",
    "# combine all DFs into a single dataframe\n",
    "sp500_ml_df = pd.concat(sp500_ml_dfs, axis = 0)\n",
    "\n",
    "# add id column\n",
    "sp500_ml_df.insert(loc = 0,\n",
    "                    column = \"ID_CIK_YEAR\",\n",
    "                    value = \"ID_\" + sp500_ml_df[\"CIK\"] + \"_\" + sp500_ml_df[\"YEAR\"])\n",
    "\n",
    "# Add target column\n",
    "sp500_ml_df[\"SP_500\"] = 1\n",
    "\n",
    "display(sp500_ml_df.shape)\n",
    "display(sp500_ml_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Write to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sp500_ml_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_SP500_part.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only NLTKly parsed\n",
    "\n",
    "sp500_ml_cleaned_df = sp500_ml_df[[\"ID_CIK_YEAR\",\"CIK\",\"YEAR\",\n",
    "                                   \"CLEANED_ITEM_1A\",\"CLEANED_ITEM_7\",\n",
    "                                   \"CLEANED_ITEM_7A\",\"SP_500\"]]\n",
    "sp500_ml_cleaned_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_SP500_Cleaned.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_10k_df[[\"CIK\",\"YEAR\",\"CLEANED_ITEM_1A\",\"CLEANED_ITEM_7\",\"CLEANED_ITEM_7A\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sp500_ml_cleaned_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Assemble All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TXT_10K_FILES = glob(os.path.join(DATA_DIR, \"sec_10k_text_files\",\"*.txt\"))\n",
    "\n",
    "all_ml_dfs = build_nlp_10k_ml_dataset(TXT_10K_FILES, start_idx=0)\n",
    "# combine all DFs into a single dataframe\n",
    "all_ml_df = pd.concat(all_ml_dfs, axis = 0)\n",
    "\n",
    "# add id column\n",
    "all_ml_df.insert(loc = 0,\n",
    "            column = \"ID_CIK_YEAR\",\n",
    "            value = \"ID_\" + all_ml_df[\"CIK\"] + \"_\" + all_ml_df[\"YEAR\"])\n",
    "\n",
    "# Add target column\n",
    "all_ml_df[\"SP_500\"] = 0\n",
    "\n",
    "display(all_ml_df.shape)\n",
    "display(all_ml_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all_ml_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_All.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# only NLTKly parsed\n",
    "\n",
    "all_ml_cleaned_df = all_ml_df[[\"ID_CIK_YEAR\",\"CIK\",\"YEAR\",\n",
    "                                   \"CLEANED_ITEM_1A\",\"CLEANED_ITEM_7\",\n",
    "                                   \"CLEANED_ITEM_7A\",\"SP_500\"]]\n",
    "all_ml_cleaned_df.to_csv(os.path.join(DATA_DIR, \"ML_Dataset\", \"SEC_10K_ALL_Cleaned.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Advanced NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "#Let's grab word counts like we did prior\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "risk_sections = [stemmer.stem(risk_section) for risk_section in risk_sections]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "counts = vectorizer.fit_transform(risk_sections)\n",
    "counts = pd.DataFrame(counts.toarray(),columns=vectorizer.get_feature_names()).transpose()\n",
    "counts.columns = [2018,2017,2016,2015,2014]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#In this case we don't need to turn it into frequency if we want to also penalize for different length risk sections\n",
    "print(sum((counts[2018]-counts[2017])**2)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We can apply the difference in a horizontal manner\n",
    "print(counts.diff(axis=1).dropna(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#And then get the distance\n",
    "print((counts.diff(axis=1).dropna(axis=1)**2).sum()**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create a function for the distance\n",
    "def find_distance(risk_sections):\n",
    "    stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    risk_sections = [stemmer.stem(risk_section) for risk_section in risk_sections]\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    counts = vectorizer.fit_transform(risk_sections)\n",
    "    counts = pd.DataFrame(counts.toarray(),columns=vectorizer.get_feature_names()).transpose()\n",
    "    counts.columns = [2018,2017,2016,2015,2014]\n",
    "    return (counts.diff(axis=1).dropna(axis=1)**2).sum()**.5\n",
    "print(find_distance(risk_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applications of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#One thing we can do is take the log of the number of terms\n",
    "tf_log = np.log(1 + counts)\n",
    "print(tf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Inverse Document Frequency\n",
    "\n",
    "#First find the number of documents with each term\n",
    "n = (counts > 0).sum(axis=1)\n",
    "\n",
    "#Then divide that by total number of documents and take the log of it\n",
    "idf = np.log(5 / n)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf = tf_log.multiply(idf, axis=0)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now we want to try cosine similarity to compare\n",
    "#Notice the interesting pattern here of similarity getting smaller and smaller as the years are farther apart\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(tf_idf.transpose())\n",
    "similarity = pd.DataFrame(similarity, index=[2018,2017,2016,2015,2014],columns=[2018,2017,2016,2015,2014])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for yr in similarity.index:\n",
    "    similarity.loc[yr].plot(kind='bar', color='blue')\n",
    "    plt.title(\"10-K Filing {}\".format(yr))\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yoy_similarity = pd.Series([similarity.loc[x,x+1] for x in [2017,2016,2015,2014]])\n",
    "yoy_similarity.index = ['{}-{}'.format(x,x+1) for x in [2017,2016,2015,2014]]\n",
    "print(yoy_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see the most impactful words that either came on in the latest or were much more frequent\n",
    "#As well as the opposite\n",
    "print(tf_idf.diff(axis=1).dropna(axis=1).iloc[:,0].sort_values().head(10))\n",
    "print()\n",
    "print()\n",
    "print(tf_idf.diff(axis=1).dropna(axis=1).iloc[:,0].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We see these words were specific only to one filing\n",
    "print(counts.loc['autonomous'])\n",
    "print()\n",
    "print(counts.loc['turkey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(counts.loc['software'])\n",
    "print()\n",
    "print(tf_idf.loc['software'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(tf_log.loc['software'])\n",
    "print()\n",
    "print(idf.loc['software'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#An application of looking at these things is to see if there are companies that are becoming competitors\n",
    "#Only in the most recent filing amazon comes up, but Oracle constantly mentions cloud\n",
    "print(counts.loc[\"amazon\"])\n",
    "print(counts.loc[\"cloud\"])\n",
    "print()\n",
    "print()\n",
    "#Because cloud is mentioned in every documents it has no importance in the tf_idf\n",
    "print(tf_idf.loc[\"amazon\"])\n",
    "print(tf_idf.loc[\"cloud\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Pull the risk sections of Amazon and Oracle\n",
    "documents_oracle = pull_10K( \"0001341439\")\n",
    "documents_amazon = pull_10K(\"0001018724\")\n",
    "\n",
    "risk_sections_oracle = [pull_risk_section(document) for document in documents_oracle]\n",
    "risk_sections_amazon = [pull_risk_section(document) for document in documents_amazon]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Parse each risk section\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "counts_oracle = vectorizer.fit_transform(risk_sections_oracle)\n",
    "counts_oracle = pd.DataFrame(counts_oracle.toarray(),columns=vectorizer.get_feature_names()).transpose()\n",
    "counts_oracle.columns = [2018,2017,2016,2015,2014]\n",
    "\n",
    "\n",
    "counts_amazon = vectorizer.fit_transform(risk_sections_amazon)\n",
    "counts_amazon = pd.DataFrame(counts_amazon.toarray(),columns=vectorizer.get_feature_names()).transpose()\n",
    "counts_amazon.columns = [2018,2017,2016,2015,2014]\n",
    "\n",
    "\n",
    "counts_amazon = counts_amazon.stack().reset_index()\n",
    "counts_oracle = counts_oracle.stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counts_amazon.columns = [\"Word\", \"Time Period\", \"Count\"]\n",
    "counts_amazon[\"Company\"] = \"Amazon\"\n",
    "counts_oracle.columns = [\"Word\", \"Time Period\", \"Count\"]\n",
    "counts_oracle[\"Company\"] = \"Oracle\"\n",
    "counts = pd.concat([counts_amazon, counts_oracle])\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Now that we are combining the two together, something to consider is how idf will be impacted\n",
    "#Before, it was a huge difference when Oracle had Amazon added to the risk section\n",
    "#Now, however, the word will look much less important if IDF is done over the full document\n",
    "print(counts[counts[\"Word\"] == \"amazon\"])\n",
    "print()\n",
    "print()\n",
    "#Likewise, Oracle as a word will now be more important because it is not featured in Amazon's 10-K\n",
    "print(counts[counts[\"Word\"] == \"oracle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counts = counts.set_index([\"Company\", \"Time Period\", \"Word\"])[\"Count\"].unstack().transpose().fillna(0)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_log = np.log(counts + 1)\n",
    "\n",
    "n = (counts > 0).sum(axis=1)\n",
    "\n",
    "idf = np.log(len(counts.columns) / n)\n",
    "tf_idf = tf_log.multiply(idf, axis=0)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's find cosine similarity\n",
    "similarity = cosine_similarity(tf_idf.transpose())\n",
    "similarity = pd.DataFrame(similarity, index=tf_idf.columns,columns=tf_idf.columns)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#And for each time period find the similarity between the two\n",
    "yearly_sim = pd.Series([similarity.loc[(\"Amazon\",x),(\"Oracle\", x)] for x in [2018,2017,2016,2015,2014]], index=[2018,2017,2016,2015,2014])\n",
    "print(yearly_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We can see that amazon talks a lot more about the supply chain buzzwords\n",
    "diff = tf_idf[(\"Amazon\", 2018)] - tf_idf[(\"Oracle\", 2018)]\n",
    "print(diff.sort_values(ascending=False).head(10))\n",
    "print()\n",
    "print()\n",
    "print(diff.sort_values().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We see that two new words that amazon began using stores, and omnichannel became a big difference\n",
    "#One limitation is that words are split\n",
    "#So if amazon mentions whole foods it would be split losing the actual importance of it being a company\n",
    "#rather than two distinct words\n",
    "i = diff.sort_values(ascending=False).head(5).index\n",
    "print(counts.loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This analysis can be useful to assess country level threats\n",
    "#And correlations in the currency markets\n",
    "print(counts.loc[['yemen', 'turkey', 'china']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#The word list has multiple sheets with tone descriptions for different words\n",
    "#Something to note is that a word can be in multiple lists!\n",
    "import pandas as pd\n",
    "word_list = pd.read_excel(\"Data/LM Word List.xlsx\", sheet_name=\"Negative\",header=None)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create a matrix of word types and the words that match these types\n",
    "word_list = []\n",
    "for sentiment_class in [\"Negative\", \"Positive\", \"Uncertainty\", \"Litigious\",\n",
    "                       \"StrongModal\", \"WeakModal\", \"Constraining\"]:\n",
    "    sentiment_list = pd.read_excel(\"Data/LM Word List.xlsx\", sheet_name=sentiment_class,header=None)\n",
    "    sentiment_list.columns = [\"Word\"]\n",
    "    sentiment_list[\"Word\"] = sentiment_list[\"Word\"].str.lower()\n",
    "    sentiment_list[sentiment_class] = 1\n",
    "    sentiment_list = sentiment_list.set_index(\"Word\")[sentiment_class]\n",
    "    word_list.append(sentiment_list)\n",
    "word_list = pd.concat(word_list, axis=1, sort=True).fillna(0)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Examples of words which have multiple groups\n",
    "print(word_list.loc[word_list.sum(axis=1) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's reindex by negative words, as well as drop na, and see what negative words are mentioned\n",
    "#We will look at percent frequency\n",
    "tf_percent = counts / counts.sum()\n",
    "negative_words = word_list[word_list[\"Negative\"] == 1].index\n",
    "negative_frequency = tf_percent.reindex(negative_words).dropna()\n",
    "print(negative_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#There seems to be slightly higher negative term frequency for amazon, especially in the last few years\n",
    "print(\"Year by Company negative word frequency\")\n",
    "print(negative_frequency.sum())\n",
    "print()\n",
    "print(\"Average company negative frequency\")\n",
    "print(negative_frequency.sum().groupby(\"Company\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see what were the most common negative words in Amazon's latest risk section\n",
    "negative_frequency.sort_values(by=(\"Amazon\", 2018), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#And for all word types.....\n",
    "l = []\n",
    "for word_type in word_list.columns:\n",
    "    word_type_list = word_list[word_list[word_type] == 1].index\n",
    "    word_type_frequency = tf_percent.reindex(word_type_list).dropna().sum()\n",
    "    l.append(word_type_frequency)\n",
    "word_type_frequency = pd.concat(l, axis=1)\n",
    "word_type_frequency.columns = word_list.columns\n",
    "print(word_type_frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
